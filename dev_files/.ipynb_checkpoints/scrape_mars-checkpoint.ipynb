{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert your Jupyter notebook into a Python script called scrape_mars.py with a function called scrape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def scrape():\n",
    "    # Dependencies\n",
    "    from splinter import Browser\n",
    "    from bs4 import BeautifulSoup\n",
    "    import requests\n",
    "    import pandas as pd\n",
    "    import pymongo\n",
    "    import time\n",
    "    import ctypes  # An included library with Python install.\n",
    "    \n",
    "    def Mbox(title, text, style):\n",
    "        return ctypes.windll.user32.MessageBoxW(0, text, title, style)\n",
    "\n",
    "    \n",
    "    mars_data_dict = {}\n",
    "    \n",
    "    ## (1) NASA Mars News\n",
    "    # Scrape the NASA Mars News Site and collect the latest News Title and Paragraph Text. \n",
    "    # Assign the text to variables that you can reference later.\n",
    "       \n",
    "    # URL of page to be scraped\n",
    "    url_nz = 'https://mars.nasa.gov/news/'\n",
    "\n",
    "    # Retrieve page with the requests module\n",
    "    response_nz = requests.get(url_nz)\n",
    "\n",
    "    # Create BeautifulSoup object; parse with 'html.parser'\n",
    "    soup_nz = BeautifulSoup(response_nz.text, 'lxml')\n",
    "\n",
    "    # Examine the results, then determine element that contains sought info\n",
    "    #print(soup_nz.prettify())\n",
    "    \n",
    "    #time.sleep(2)\n",
    "    \n",
    "    # Find the latest News Title\n",
    "    news_title = soup_nz.find(\"div\", class_=\"content_title\").a.text[1:-1]\n",
    "    #print(news_title)\n",
    "    \n",
    "    # Find the latest News Paragraph Text\n",
    "    news_p = soup_nz.find(\"div\", class_=\"image_and_description_container\").a.text[3:-7]\n",
    "    #print(news_p)\n",
    "    \n",
    "    mars_data_dict[\"news_title\"] = news_title\n",
    "    mars_data_dict[\"news_p\"] = news_p\n",
    "        \n",
    "        \n",
    "    \n",
    "    ## (2) JPL Mars Space Images - Featured Image\n",
    "    # Use splinter to navigate the site and find the image url for the current Featured Mars Image \n",
    "    # and assign the url string to a variable called featured_image_url.\n",
    "    # Make sure to find the image url to the full size .jpg image.\n",
    "    # Make sure to save a complete url string for this image.\n",
    "    \n",
    "    executable_path = {'executable_path': 'chromedriver.exe'}\n",
    "    browser = Browser('chrome', **executable_path, headless=False)\n",
    "\n",
    "    # URL of page to be scraped\n",
    "    url_jpl = 'https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars'\n",
    "    browser.visit(url_jpl)\n",
    "    \n",
    "    time.sleep(2)\n",
    "    \n",
    "    #dir(browser)\n",
    "    \n",
    "    browser.click_link_by_id('full_image')\n",
    "    \n",
    "    time.sleep(2)\n",
    "    \n",
    "    browser.click_link_by_partial_href(\"/spaceimages/details.\")\n",
    "    \n",
    "    time.sleep(2)\n",
    "    \n",
    "    browser.click_link_by_partial_href(\"/spaceimages/images/largesize\")\n",
    "    \n",
    "    time.sleep(2)\n",
    "    \n",
    "    featured_image_url = browser.url\n",
    "    #print(featured_image_url)\n",
    "    \n",
    "    mars_data_dict[\"feat_img\"] = featured_image_url\n",
    "    \n",
    "    browser.quit()\n",
    "    \n",
    "           \n",
    "    \n",
    "    ## (3) Mars Weather\n",
    "    # Visit the Mars Weather twitter account here and scrape the latest Mars weather tweet from the page.\n",
    "    # Save the tweet text for the weather report as a variable called mars_weather.\n",
    "        \n",
    "    # URL of page to be scraped\n",
    "    url_tweet = 'https://twitter.com/marswxreport?lang=en'\n",
    "\n",
    "    # Retrieve page with the requests module\n",
    "    response_tweet = requests.get(url_tweet)\n",
    "\n",
    "    # Create BeautifulSoup object; parse with 'html.parser'\n",
    "    soup_tweet = BeautifulSoup(response_tweet.text, 'lxml')\n",
    "\n",
    "    # Examine the results, then determine element that contains sought info\n",
    "    #print(soup_tweet.prettify())\n",
    "    \n",
    "    #time.sleep(2)\n",
    "    \n",
    "    # scrape the latest Mars weather tweet from the page\n",
    "    tweets = soup_tweet.find_all(\"p\", class_=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\")\n",
    "    for tweet in tweets:\n",
    "        find_text = tweet.text.find(\"InSight sol\")\n",
    "        if find_text == 0:\n",
    "            mars_weather = tweet.text\n",
    "            #print(mars_weather)\n",
    "            break\n",
    "    \n",
    "    mars_data_dict[\"weather\"] = mars_weather\n",
    "    \n",
    "    \n",
    "    \n",
    "    ## (4) Mars Facts\n",
    "    # URL of page to be scraped\n",
    "    url_mfacts = 'https://space-facts.com/mars/'\n",
    "\n",
    "    # Retrieve page with the requests module\n",
    "    response_mfacts = requests.get(url_mfacts)\n",
    "\n",
    "    # Create BeautifulSoup object; parse with 'html.parser'\n",
    "    soup_mfacts = BeautifulSoup(response_mfacts.text, 'lxml')\n",
    "\n",
    "    # Examine the results, then determine element that contains sought info\n",
    "    #print(soup_mfacts.prettify())\n",
    "    \n",
    "    #time.sleep(2)\n",
    "    \n",
    "    tables = pd.read_html(url_mfacts)[1]\n",
    "    #tables\n",
    "    \n",
    "    mars_data_dict[\"mfacts\"] = tables\n",
    "    \n",
    "    tables.to_html(\"../html/mars_facts.html\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    ## (5) Mars Hemispheres\n",
    "    # Visit the USGS Astrogeology site here to obtain high resolution images for each of Mar's hemispheres.\n",
    "    # You will need to click each of the links to the hemispheres in order to find the image url to the full resolution image.\n",
    "    # Save both the image url string for the full resolution hemisphere image, \n",
    "    #     and the Hemisphere title containing the hemisphere name. Use a Python dictionary to store the data using the \n",
    "    #     keys img_url and title.\n",
    "    # Append the dictionary with the image url string and the hemisphere title to a list. \n",
    "    #     This list will contain one dictionary for each hemisphere\n",
    "    \n",
    "    executable_path = {\"executable_path\": \"chromedriver.exe\"}\n",
    "    browser = Browser(\"chrome\", **executable_path, headless=False)\n",
    "\n",
    "    # URL of page to be scraped\n",
    "    url_mhemi = \"https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars\"\n",
    "    browser.visit(url_mhemi)\n",
    "    \n",
    "    time.sleep(2)\n",
    "    \n",
    "    # Image 1\n",
    "    browser.click_link_by_partial_text(\"Cerberus Hemisphere Enhanced\")\n",
    "    \n",
    "    time.sleep(2) \n",
    "    \n",
    "    title1 = browser.title.split(\"|\")[0]\n",
    "    #print(title1)\n",
    "    \n",
    "    browser.click_link_by_text(\"Sample\")\n",
    "    \n",
    "    time.sleep(2)\n",
    "    \n",
    "    img1_url = browser.windows[1].url\n",
    "    #print(img1_url)\n",
    "    \n",
    "    time.sleep(2) \n",
    "    \n",
    "    browser.windows[1].close()\n",
    "    browser.back()\n",
    "    \n",
    "    hemi1_dict = {}\n",
    "    hemi1_dict[\"title\"] = title1\n",
    "    hemi1_dict[\"img_url\"] = img1_url\n",
    "    #hemi1_dict\n",
    "    \n",
    "    # Image 2\n",
    "    \n",
    "    browser.click_link_by_partial_text(\"Schiaparelli Hemisphere Enhanced\")\n",
    "    \n",
    "    time.sleep(2)\n",
    "    \n",
    "    title2 = browser.title.split(\"|\")[0]\n",
    "    #print(title2)\n",
    "    \n",
    "    browser.click_link_by_text(\"Sample\")\n",
    "    \n",
    "    time.sleep(2)\n",
    "    \n",
    "    img2_url = browser.windows[1].url\n",
    "    #print(img2_url)\n",
    "    \n",
    "    time.sleep(2)\n",
    "    \n",
    "    browser.windows[1].close()\n",
    "    browser.back()\n",
    "    \n",
    "    hemi2_dict = {}\n",
    "    hemi2_dict[\"title\"] = title2\n",
    "    hemi2_dict[\"img_url\"] = img2_url\n",
    "    #hemi2_dict\n",
    "    \n",
    "    # Image 3\n",
    "    \n",
    "    browser.click_link_by_partial_text(\"Syrtis Major Hemisphere Enhanced\")\n",
    "    \n",
    "    time.sleep(2)\n",
    "    \n",
    "    title3 = browser.title.split(\"|\")[0]\n",
    "    #print(title3)\n",
    "    \n",
    "    browser.click_link_by_text(\"Sample\")\n",
    "    \n",
    "    time.sleep(2)\n",
    "    \n",
    "    img3_url = browser.windows[1].url\n",
    "    #print(img3_url)\n",
    "    \n",
    "    time.sleep(2)\n",
    "    \n",
    "    browser.windows[1].close()\n",
    "    browser.back()\n",
    "    \n",
    "    hemi3_dict = {}\n",
    "    hemi3_dict[\"title\"] = title3\n",
    "    hemi3_dict[\"img_url\"] = img3_url\n",
    "    #hemi3_dict\n",
    "    \n",
    "    # Image 4\n",
    "    browser.click_link_by_partial_text(\"Valles Marineris Hemisphere Enhanced\")\n",
    "    \n",
    "    time.sleep(2)\n",
    "    \n",
    "    title4 = browser.title.split(\"|\")[0]\n",
    "    #print(title4)\n",
    "    \n",
    "    browser.click_link_by_text(\"Sample\")\n",
    "    \n",
    "    time.sleep(2)\n",
    "    \n",
    "    img4_url = browser.windows[1].url\n",
    "    #print(img4_url)\n",
    "    \n",
    "    time.sleep(2)\n",
    "    \n",
    "    browser.windows[1].close()\n",
    "    browser.back()\n",
    "    \n",
    "    hemi4_dict = {}\n",
    "    hemi4_dict[\"title\"] = title4\n",
    "    hemi4_dict[\"img_url\"] = img4_url\n",
    "    #hemi4_dict\n",
    "    \n",
    "    hemisphere_image_urls = [hemi1_dict, hemi2_dict, hemi3_dict, hemi4_dict]\n",
    "    #hemisphere_image_urls\n",
    "    \n",
    "    mars_data_dict[\"hemi_img\"] = hemisphere_image_urls\n",
    "    mars_data_dict  \n",
    "    \n",
    "    browser.quit()\n",
    "    \n",
    "\n",
    "    \n",
    "    Mbox(\"Mission to Mars Completed\", \"Congratulations!!! You've mined Mars!\", 1)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
